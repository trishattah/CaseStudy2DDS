---
title: "DDSAnalytics Predictive Model DS6306"
#author: "Patricia Attah"
date: "8/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#install.packages('olsrr')
library(MASS)
library(dplyr)
library(e1071)
library(class)
library(caret)
library(Metrics)
library(readxl)

CS <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/R-SMU/CaseStudy2_2_2_2_2_2_2/CaseStudy2-data.csv")

CS2 <- CS
CS3 <- CS

CaseStudy2CompSet_No_Salary <- read_excel("/Users/patriciaattah/Library/Mobile Documents/com~apple~CloudDocs/R-SMU/CaseStudy2_2_2_2_2_2_2/CaseStudy2CompSet No Salary.xlsx")

CaseStudy2CompSet.No.Attrition <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/R-SMU/CaseStudy2_2_2_2_2_2_2/CaseStudy2CompSet No Attrition.csv")
```
## Executive Summary

TDDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 100 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business greenlights the project, they have tasked your data science team to conduct an analysis of existing employee data.

## Introduction

This project focuses on using machine learning and algorithms to capture the factors that affect attrition and montlhy income from the available data that is provided in the data set used for this study.. The employee data consists of 860 rows and 36 variables there are 8 factored variables while the rest was numeric.There were two models created a classifier model using knn for the prediction of attrition and a linear regression model that gives us how much certain variables are effective for predicting income. after preliminary run of foward and backward regression on some variables that were already preselected they were narrowed down to three both for the knn classifier and the linear regrssion model.

#### Youtube link
https://www.youtube.com/watch?v=0wkQlpAy9XU
 

### Attrition Prediction
This is a look into the factors causing attrition. the variables TotalWorkingYears, JobInvolvement and OverTime were found to be adequate for classification, with a Sensitivity of 0.8952 and Specificity of 0.8750.    
```{r, include=FALSE}
CS_NA <- CaseStudy2CompSet.No.Attrition

CS_NA$OverTime  <- as.numeric(CS_NA$OverTime) 
CS2$OverTime <- as.numeric(CS2$OverTime)
```


```{r}

splitPerc =.75
set.seed(7)
trainIndices = sample(1:dim(CS2)[1],round(splitPerc * dim(CS2)[1]))
trainCS2_A = CS2[trainIndices,]
testCS2_A = CS2[-trainIndices,]

classifications = knn(trainCS2_A[,c('TotalWorkingYears','JobInvolvement','OverTime')] ,testCS2_A[,c('TotalWorkingYears','JobInvolvement' ,'OverTime')],trainCS2_A$Attrition, prob = TRUE, k = 5)
table(testCS2_A$Attrition,classifications)
confusionMatrix(table(testCS2_A$Attrition,classifications))

classifications2 = knn(trainCS2_A[,c('TotalWorkingYears','JobInvolvement','OverTime')] ,CS_NA[,c('TotalWorkingYears','JobInvolvement' ,'OverTime')],trainCS2_A$Attrition, prob = TRUE, k = 5)

CS_NA$Predict_Attrition <- classifications2
Attrition_pred <- CS_NA %>% dplyr::select(ID,Predict_Attrition)
```
### Income Prediction
This is a look into the factors for income prediction. First EDA is conducted on the data to check for correlation and confoundong factors. 

### Statistical Check
## Multiple Linear regression Assumptions
1. Linearity 
2. Constant Variance
3. Normality 
4. Independence.
From box plots we can see that constant variance and linearity seems to be satisfied. Normality is satisfied by large sample size. there seems to be some correlation between total working years and job level however we see that these two variables are still very effective in the model.

```{r, warning=FALSE}
CS2$Department <- as.factor(CS2$Department)
boxplot(CS2$MonthlyIncome~CS2$Department, main=toupper("MonthlyIncome by Department"), font.main=3, cex.main=1.2, xlab="Department", ylab="MonthlyIncome", font.lab=3, col="darkgreen")

boxplot(CS2$MonthlyIncome~CS2$JobLevel, main=toupper("MonthlyIncome by JobLevel"), font.main=3, cex.main=1.2, xlab="JobLevel", ylab="MonthlyIncome", font.lab=3, col="darkgreen")

```
### Pearson Correlation

```{r}
CS2$Department <- as.numeric(CS2$Department)
CS2$JobLevel <- as.numeric(CS2$JobLevel)
a = CS2 %>% dplyr::select(Department, JobLevel,TotalWorkingYears)
cor(a)

```


### Lack of Fit Test
A lack of fit test was conducted on the department varaible to check if the linear model well fitted to the seperate means model and it was found to be similar, hence the linear regrssion model is appropriate for this varibable in the model. P value was found to be 0.18 which is higher than the alpha level of 0.05

```{r}
Reduced <- lm(MonthlyIncome ~ Department, data = CS2)
Full <- lm(MonthlyIncome ~ 0 + as.factor(Department), data = CS2)
anova (Reduced,Full)

```

### RMSE
After the analysis we found the RMSE price to be $1371.55

````{r, warning=FALSE}

CS_NS <- CaseStudy2CompSet_No_Salary
CS_NS$Department <- as.factor(CS_NS$Department)


model2 <- lm(MonthlyIncome ~ TotalWorkingYears+Department +JobLevel, data = CS3)
rmse(CS3$MonthlyIncome, predict(model2, CS3))
```

### Residual Check
Checking the residual plot we can see the residuals are evenly spread, the qq plot shows there is no eveidence against normality, checking the residual plot there is no point that has both high leverage and high influence so there are no outliers, the studentised residuals are also even.

```{r}
CS2I_pred = predict(model2, interval = 'predict', newdata = CS_NS) 
CS_NS$Predict_Income <- CS2I_pred[,1]

Predict_Income <- CS_NS %>% dplyr::select(ID,Predict_Income)

par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(model2)

```

### Multiple Regression Model
The most significant variable for this model was job level followed by total working years, it was found that the sub-level sales in department provided low significance as compared to its relevance to the reference Department Human resources.
However the overall model proved to be significant with a p value of <0.0001 and an adjusted R^2 of 0.9105.
```{r}
summary(model2)

```


### Job Role specific Trends
We can see that it seems sales executives have the best work life balance as they are the highest both in the better and best levels of this category.
In the second plot we see that research scientist are the one that work most over time and sales representatives are following closely behind. 
Once again we see sales executives as the most involved in their jobs as the they are by far the highest in this area.
and finally Sales executives are the most satisfied with their jobs with research scientist following closely behind

```{r}

W_CS3 <- CS3 %>% 
 count(JobRole, WorkLifeBalance) %>% 
 group_by(JobRole) %>% 
 transmute(WorkLifeBalance, Percentage=n/sum(n)*100)

ggplot(W_CS3, aes(fill=JobRole, y=Percentage, x=WorkLifeBalance)) + 
    geom_bar(position="dodge", stat="identity")

O_CS3 <- CS3 %>% 
 count(JobRole, OverTime) %>% 
 group_by(JobRole) %>% 
 transmute(OverTime, Percentage=n/sum(n)*100)

ggplot(O_CS3, aes(fill=JobRole, y=Percentage, x=OverTime)) + 
    geom_bar(position="dodge", stat="identity")

J_CS3 <- CS3 %>% 
 count(JobRole, JobInvolvement) %>% 
 group_by(JobRole) %>% 
 transmute(JobInvolvement, Percentage=n/sum(n)*100)

ggplot(J_CS3, aes(fill=JobRole, y=Percentage, x=JobInvolvement)) + 
    geom_bar(position="dodge", stat="identity")

B_CS3 <- CS3 %>% 
 count(JobRole, JobSatisfaction) %>% 
 group_by(JobRole) %>% 
 transmute(JobSatisfaction, Percentage=n/sum(n)*100)

ggplot(B_CS3, aes(fill=JobRole, y=Percentage, x=JobSatisfaction)) + 
    geom_bar(position="dodge", stat="identity")

```



```{r, include=FALSE}
#Predict_Income
#str(CS)
#ols_step_both_p(model2)
#CS_NS$Department <- as.numeric(CS_NS$Department)
#CS2 <- CS2 %>% filter(TotalWorkingYears != 40)
#out = boxplot(CS2$NumCompaniesWorked)$out

#boxplot(CS2$JobLevel)
#boxplot(CS2$TotalWorkingYears)
#out = boxplot(CS2$TotalWorkingYears)$out
#which(CS2$TotalWorkingYears %in% out)
#out

#tab <- table(CS2$Department, CS2$JobLevel)
#chisq.test(tab)
str(CS2)

#ggplot(CS2, aes(x=MonthlyIncome, y=TotalWorkingYears, color=Department)) +
  #geom_point()+
  #geom_smooth(method=lm, se=FALSE, fullrange=TRUE)

#ggplot(CS2, aes(x=JobRole, y= YearsAtCompany, fill=JobRole)) +
  #geom_boxplot()

write.csv(Predict_Income,"/Users/patriciaattah/Library/Mobile Documents/com~apple~CloudDocs/R-SMU/Predict_Income.csv", row.names = FALSE)

write.csv(Attrition_pred,"/Users/patriciaattah/Library/Mobile Documents/com~apple~CloudDocs/R-SMU/Attrition_pred.csv", row.names = FALSE)
```

